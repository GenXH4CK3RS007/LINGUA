# Lingua 
---
![Homepage view 1](https://github.com/user-attachments/assets/6bda9ca0-0276-436b-9940-7490677a062a)


 L: Live â€“ Real-time translation of ISL into text and speech. <br/>
<br/>
 I: Interpretation â€“ Accurate recognition and interpretation of Indian Sign Language. <br/>
<br/>
 N: Narration â€“ Providing speech output in multiple Indian languages. <br/>
<br/>
 G: Gesture â€“ Recognizing and processing hand gestures and signs. <br/>
<br/>
 U: Understanding â€“ Facilitating communication and understanding between the deaf/hard-of-hearing and the hearing world. <br/>
<br/>
 A: Accessibility â€“ Making communication more accessible for the deaf and hard-of-hearingÂ community. <br/>
<br/>

Indian Sign Language to Text/Speech translation (For SIH2024)
-
P.S : To create a solution that translates Indian Sign Language (ISL) into text and speech in real-time, facilitating communication for the deaf and hard-of-hearing community with the hearing world. <br/> 
<br/>
The application is capable of recognizing and interpreting a comprehensive library of ISL signs and gestures, and then provide accurate text and speech output in multiple Indian languages.



Proposed solution for round 1 internal hackathon (SIH2024)
-
A smart glove, that's capable of sensing the motion made by the fingers, through the flex sensor / gyroscopic(axis) sensor
-

Data workflow
-

![WhatsApp Image 2024-09-05 at 12 45 25_5f6d03d0](https://github.com/user-attachments/assets/54b9f4be-b438-4aa9-9799-7afbba3dd963)


Pin Diagram:-
-
![WhatsApp Image 2024-09-05 at 13 25 26_88a407de](https://github.com/user-attachments/assets/35bf75df-062b-40f5-8ca1-7d63e7a7a197)


Hardware techstack:-
-
![WhatsApp Image 2024-09-05 at 12 54 25_d4890245](https://github.com/user-attachments/assets/d97ce190-6987-416c-9a9c-f0f36cd8273a)


ISL Replica testing (Video) //ML model
-

//work in progress
......... //put asl working video demo here

Hardware implementation (physical video) 
-
//work in progress
.........




![navigation view 1](https://github.com/user-attachments/assets/247e25a2-e5a6-4736-882a-5a49c5a0063b)

American Sign Language Code architecture testing:- (checking the media pipe algorithm, Computer vision, NLP Suggestions, Text output, Text output to speech through various TTS models, like gTTS) <br/>
-
//in plans to achieve a similar output with ISL Software, cross checking all the dependencies..<br/>
-

https://github.com/user-attachments/assets/618fc4ac-aba9-44da-8c69-c3c5e13a91da



Future aspects (explaining it on a layman's view, keeping it completely non technical) <br/>
-

=> Smart wearable gloves that facilitates wireless connection via a bluetooth external device (in that case go for raspberry pi, instead of arduino) <br/>
=> Smart wearable glasses, that displays text as running words coming from both the text generation, as well as the NLP suggestion <br/>
=> Achieving Corpus in sign language, checking grammar, punctuation, stop word removal and removing ambiguities (syntactic parsing),Â inÂ NLPs...(Usually happens after the data preprocessing step, in Model training) <br/>
=> Acheiving TTS model training epochs from scratch, instead of calling API calls only to get the robotic AI voices from Chatgpt, OpenAI, and other software sources like seamless and lovo ai, helping us to leverage voice integration modules of humanised vocal versions..<br/>
//
smtg to look outðŸ‘€ (Developer resources, that i found useful) <br/>
https://lovo.ai/ <br/>
https://genny.lovo.ai/signup  <br/>
genny api setup <br/>
<br/>
![WhatsApp Image 2024-09-04 at 22 21 53_efd85b5b](https://github.com/user-attachments/assets/544b895e-7c69-4dde-899f-60e05679a0ba) <br/>

A sample tutorial could be accessed here:-
-

https://lovo.ai/tutorials/using-genny-api/getting-started-with-genny-api





